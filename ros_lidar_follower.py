#!/usr/bin/env python3

import rospy
import cv2
import numpy as np
import time
import os
import json
import math
from enum import Enum
import requests

from jetbot import Robot
import onnxruntime as ort
from pyzbar.pyzbar import decode
import paho.mqtt.client as mqtt
from sensor_msgs.msg import LaserScan, Image
from opposite_detector import SimpleOppositeDetector

from map_navigator import MapNavigator

class RobotState(Enum):
    DRIVING_STRAIGHT = 1
    LINE_VALIDATION = 1.5
    APPROACHING_INTERSECTION = 2
    HANDLING_EVENT = 3
    LEAVING_INTERSECTION = 4
    REACQUIRING_LINE = 5
    DEAD_END = 6
    GOAL_REACHED = 7

class Direction(Enum):
    NORTH, EAST, SOUTH, WEST = 0, 1, 2, 3

class JetBotController:
    def __init__(self):
        rospy.loginfo("ƒêang kh·ªüi t·∫°o JetBot Event-Driven Controller...")
        self.setup_parameters()
        self.initialize_hardware()
        self.initialize_yolo()
        self.initialize_mqtt()

        self.video_writer = None
        self.initialize_video_writer()

        self.navigator = MapNavigator(self.MAP_FILE_PATH)
        self.current_node_id = self.navigator.start_node
        self.target_node_id = None
        self.planned_path = None
        self.banned_edges = []
        self.plan_initial_route()

        self.latest_scan = None
        self.latest_image = None
        self.detector = SimpleOppositeDetector()
        
        # Initialize angle analyzer for line/lidar comparison
        self.angle_analysis_enabled = True
        self.last_angle_analysis_time = 0
        self.angle_analysis_interval = 2.0  # Analyze every 2 seconds
        
        # Camera-based intersection detection
        self.camera_intersection_detected = False
        self.camera_detection_time = 0
        self.waiting_for_lidar_confirmation = False
        self.lidar_confirmation_timeout = 4.0  # 4 seconds to wait for LiDAR
        
        # Flag-based start control
        self.initial_red_flag_detected = False  # Track if red flag was detected at start
        self.robot_started = False              # Track if robot has started moving
        
        rospy.Subscriber('/scan', LaserScan, self.detector.callback)
        rospy.Subscriber('/csi_cam_0/image_raw', Image, self.camera_callback)
        rospy.loginfo("ƒê√£ ƒëƒÉng k√Ω v√†o c√°c topic /scan v√† /csi_cam_0/image_raw.")
        self.state_change_time = rospy.get_time()
        self.line_validation_attempts = 0  # Counter cho LINE_VALIDATION state
        self._set_state(RobotState.DRIVING_STRAIGHT, initial=True)
        rospy.loginfo("Kh·ªüi t·∫°o ho√†n t·∫•t. S·∫µn s√†ng ho·∫°t ƒë·ªông.")

    def plan_initial_route(self): 
        """L·∫≠p k·∫ø ho·∫°ch ƒë∆∞·ªùng ƒëi ban ƒë·∫ßu t·ª´ ƒëi·ªÉm xu·∫•t ph√°t ƒë·∫øn ƒë√≠ch."""
        rospy.loginfo(f"ƒêang l·∫≠p k·∫ø ho·∫°ch t·ª´ node {self.navigator.start_node} ƒë·∫øn {self.navigator.end_node}...")
        self.planned_path = self.navigator.find_path(
            self.navigator.start_node, 
            self.navigator.end_node,
            self.banned_edges
        )
        if self.planned_path and len(self.planned_path) > 1:
            self.target_node_id = self.planned_path[1]
            rospy.loginfo(f"ƒê√£ t√¨m th·∫•y ƒë∆∞·ªùng ƒëi: {self.planned_path}. ƒê√≠ch ƒë·∫øn ƒë·∫ßu ti√™n: {self.target_node_id}")
        else:
            rospy.logerr("Kh√¥ng t√¨m th·∫•y ƒë∆∞·ªùng ƒëi ho·∫∑c ƒë∆∞·ªùng ƒëi qu√° ng·∫Øn!")
            self._set_state(RobotState.DEAD_END)

    def initialize_video_writer(self):
        """Kh·ªüi t·∫°o ƒë·ªëi t∆∞·ª£ng VideoWriter."""
        try:
            # K√≠ch th∆∞·ªõc video s·∫Ω gi·ªëng k√≠ch th∆∞·ªõc ·∫£nh robot x·ª≠ l√Ω
            frame_size = (self.WIDTH, self.HEIGHT)
            self.video_writer = cv2.VideoWriter(self.VIDEO_OUTPUT_FILENAME, 
                                                self.VIDEO_FOURCC, 
                                                self.VIDEO_FPS, 
                                                frame_size)
            if self.video_writer.isOpened():
                rospy.loginfo(f"B·∫Øt ƒë·∫ßu ghi video v√†o file '{self.VIDEO_OUTPUT_FILENAME}'")
            else:
                rospy.logerr("Kh√¥ng th·ªÉ m·ªü file video ƒë·ªÉ ghi.")
                self.video_writer = None
        except Exception as e:
            rospy.logerr(f"L·ªói khi kh·ªüi t·∫°o VideoWriter: {e}")
            self.video_writer = None

    def draw_debug_info(self, image):
        """V·∫Ω c√°c th√¥ng tin g·ª° l·ªói l√™n m·ªôt khung h√¨nh."""
        if image is None:
            return None
        
        debug_frame = image.copy()
        
        # 1. V·∫Ω c√°c ROI
        # ROI Ch√≠nh (m√†u xanh l√°)
        cv2.rectangle(debug_frame, (0, self.ROI_Y), (self.WIDTH-1, self.ROI_Y + self.ROI_H), (0, 255, 0), 1)
        # ROI D·ª± b√°o (m√†u v√†ng)
        cv2.rectangle(debug_frame, (0, self.LOOKAHEAD_ROI_Y), (self.WIDTH-1, self.LOOKAHEAD_ROI_Y + self.LOOKAHEAD_ROI_H), (0, 255, 255), 1)

        # 2. V·∫Ω tr·∫°ng th√°i hi·ªán t·∫°i
        state_text = f"State: {self.current_state.name if self.current_state else 'None'}"
        cv2.putText(debug_frame, state_text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        
        # 3. V·∫Ω line v√† tr·ªçng t√¢m (n·∫øu robot ƒëang b√°m line)
        if self.current_state == RobotState.DRIVING_STRAIGHT:
            # L·∫•y line center c·ªßa ROI Ch√≠nh
            line_center = self._get_line_center(image, self.ROI_Y, self.ROI_H)
            if line_center is not None:
                # V·∫Ω m·ªôt ƒë∆∞·ªùng th·∫≥ng ƒë·ª©ng m√†u ƒë·ªè t·∫°i v·ªã tr√≠ tr·ªçng t√¢m
                cv2.line(debug_frame, (line_center, self.ROI_Y), (line_center, self.ROI_Y + self.ROI_H), (0, 0, 255), 2)

        return debug_frame

    def setup_parameters(self):
        self.WIDTH, self.HEIGHT = 300, 300
        self.BASE_SPEED = 0.2
        self.TURN_SPEED = 0.2
        self.TURN_DURATION_90_DEG = 0.8
        self.ROI_Y = int(self.HEIGHT * 0.85)
        self.ROI_H = int(self.HEIGHT * 0.2)
        self.ROI_CENTER_WIDTH_PERCENT = 0.5
        self.LOOKAHEAD_ROI_Y = int(self.HEIGHT * 0.75) # V·ªã tr√≠ Y cao h∆°n
        self.LOOKAHEAD_ROI_H = int(self.HEIGHT * 0.15) # Chi·ªÅu cao t∆∞∆°ng t·ª±

        self.CORRECTION_GAIN = 0.5
        self.SAFE_ZONE_PERCENT = 0.3
        self.LINE_COLOR_LOWER = np.array([0, 0, 0])
        self.LINE_COLOR_UPPER = np.array([180, 255, 120])  # Increased Value from 75 to 120 for better black line detection
        self.INTERSECTION_CLEARANCE_DURATION = 1.5
        self.INTERSECTION_APPROACH_DURATION = 0.5
        self.LINE_REACQUIRE_TIMEOUT = 3.0
        self.SCAN_PIXEL_THRESHOLD = 100
        self.YOLO_MODEL_PATH = "models/best.onnx"
        self.YOLO_CONF_THRESHOLD = 0.6
        self.YOLO_INPUT_SIZE = (640, 640)
        self.YOLO_CLASS_NAMES = ['N', 'E', 'W', 'S', 'NN', 'NE', 'NW', 'NS', 'math']
        self.PRESCRIPTIVE_SIGNS = {'N', 'E', 'W', 'S'}
        self.PROHIBITIVE_SIGNS = {'NN', 'NE', 'NW', 'NS'}
        self.DATA_ITEMS = {'qr_code', 'math_problem'}
        self.MQTT_BROKER = "localhost"; self.MQTT_PORT = 1883
        self.MQTT_DATA_TOPIC = "jetbot/corrected_event_data"
        self.current_state = None
        self.DIRECTIONS = [Direction.NORTH, Direction.EAST, Direction.SOUTH, Direction.WEST]
        self.current_direction_index = 1
        self.ANGLE_TO_FACE_SIGN_MAP = {d: a for d, a in zip(self.DIRECTIONS, [45, -45, -135, 135])}
        self.MAX_CORRECTION_ADJ = 0.12
        self.MAP_FILE_PATH = "map_z"
        self.LABEL_TO_DIRECTION_ENUM = {'N': Direction.NORTH, 'E': Direction.EAST, 'S': Direction.SOUTH, 'W': Direction.WEST}
        self.VIDEO_OUTPUT_FILENAME = 'jetbot_run.avi'
        self.VIDEO_FPS = 20  # N√™n kh·ªõp v·ªõi rospy.Rate c·ªßa b·∫°n
        # Codec 'MJPG' r·∫•t ph·ªï bi·∫øn v√† t∆∞∆°ng th√≠ch t·ªët
        self.VIDEO_FOURCC = cv2.VideoWriter_fourcc(*'MJPG')
        
        # Parameters cho LINE_VALIDATION state
        self.LINE_VALIDATION_TIMEOUT = 2.0  # Th·ªùi gian t·ªëi ƒëa ƒë·ªÉ validate line position
        self.LINE_CENTER_TOLERANCE = 0.2    # T·ª∑ l·ªá cho ph√©p line l·ªách kh·ªèi center (20% width)
        self.LINE_VALIDATION_ATTEMPTS = 8   # S·ªë l·∫ßn th·ª≠ validate t·ªëi ƒëa
        
        # Parameters cho Camera-LiDAR Intersection Detection
        self.CAMERA_LIDAR_INTERSECTION_MODE = True  # Enable camera-first detection
        self.CROSS_DETECTION_ROI_Y_PERCENT = 0.45   # Extended from 0.50 to 0.45 - detect earlier
        self.CROSS_DETECTION_ROI_H_PERCENT = 0.30   # Extended from 0.20 to 0.30 - larger detection area
        self.CROSS_MIN_ASPECT_RATIO = 1.2           # Reduced from 1.5 to 1.2 - catch even thinner cross lines
        self.CROSS_MIN_WIDTH_RATIO = 0.25           # Reduced from 0.3 to 0.25 - catch even shorter cross lines
        self.CROSS_MAX_HEIGHT_RATIO = 0.8           # Height ratio t·ªëi ƒëa so v·ªõi ROI
        
        # Flag detection parameters - Improved for better red detection
        self.FLAG_RED_LOWER1 = np.array([0, 50, 50])     # Lower red range
        self.FLAG_RED_UPPER1 = np.array([10, 255, 255])
        self.FLAG_RED_LOWER2 = np.array([170, 50, 50])   # Upper red range  
        self.FLAG_RED_UPPER2 = np.array([180, 255, 255])
        self.FLAG_COVERAGE_THRESHOLD = 0.15  # Reduced from 0.3 to 0.15 for easier detection

    def initialize_hardware(self):
        try:
            self.robot = Robot()
            rospy.loginfo("Ph·∫ßn c·ª©ng JetBot (ƒë·ªông c∆°) ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o.")
        except Exception as e:
            rospy.logwarn(f"Kh√¥ng t√¨m th·∫•y ph·∫ßn c·ª©ng JetBot, s·ª≠ d·ª•ng Mock object. L·ªói: {e}")
            from unittest.mock import Mock
            self.robot = Mock()

    def initialize_yolo(self):
        """T·∫£i m√¥ h√¨nh YOLO v√†o ONNX Runtime."""
        try:
            self.yolo_session = ort.InferenceSession(self.YOLO_MODEL_PATH, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
            rospy.loginfo("T·∫£i m√¥ h√¨nh YOLO th√†nh c√¥ng.")
        except Exception as e:
            rospy.logerr(f"Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh YOLO t·ª´ '{self.YOLO_MODEL_PATH}'. L·ªói: {e}")
            self.yolo_session = None

    def numpy_nms(self, boxes, scores, iou_threshold):
        """
        Th·ª±c hi·ªán Non-Maximum Suppression (NMS) b·∫±ng NumPy.
        :param boxes: list c√°c bounding box, m·ªói box l√† [x1, y1, x2, y2]
        :param scores: list c√°c ƒëi·ªÉm tin c·∫≠y t∆∞∆°ng ·ª©ng
        :param iou_threshold: ng∆∞·ª°ng IoU ƒë·ªÉ lo·∫°i b·ªè c√°c box tr√πng l·∫∑p
        :return: list c√°c ch·ªâ s·ªë (indices) c·ªßa c√°c box ƒë∆∞·ª£c gi·ªØ l·∫°i
        """
        # Chuy·ªÉn ƒë·ªïi sang NumPy array ƒë·ªÉ t√≠nh to√°n vector h√≥a
        x1 = np.array([b[0] for b in boxes])
        y1 = np.array([b[1] for b in boxes])
        x2 = np.array([b[2] for b in boxes])
        y2 = np.array([b[3] for b in boxes])

        areas = (x2 - x1 + 1) * (y2 - y1 + 1)
        # S·∫Øp x·∫øp c√°c box theo ƒëi·ªÉm tin c·∫≠y gi·∫£m d·∫ßn
        order = scores.argsort()[::-1]

        keep = []
        while order.size > 0:
            i = order[0]
            keep.append(i)
            
            # T√≠nh to√°n IoU (Intersection over Union)
            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])

            w = np.maximum(0.0, xx2 - xx1 + 1)
            h = np.maximum(0.0, yy2 - yy1 + 1)
            intersection = w * h
            
            iou = intersection / (areas[i] + areas[order[1:]] - intersection)

            # Gi·ªØ l·∫°i c√°c box c√≥ IoU nh·ªè h∆°n ng∆∞·ª°ng
            inds = np.where(iou <= iou_threshold)[0]
            order = order[inds + 1]

        return np.array(keep)

    def detect_with_yolo(self, image):
        """
        Th·ª±c hi·ªán nh·∫≠n di·ªán ƒë·ªëi t∆∞·ª£ng b·∫±ng YOLOv8 v√† h·∫≠u x·ª≠ l√Ω k·∫øt qu·∫£ ƒë√∫ng c√°ch.
        """
        if self.yolo_session is None: return []

        original_height, original_width = image.shape[:2]

        img_resized = cv2.resize(image, self.YOLO_INPUT_SIZE)
        img_data = np.array(img_resized, dtype=np.float32) / 255.0
        img_data = np.transpose(img_data, (2, 0, 1))  # HWC to CHW
        input_tensor = np.expand_dims(img_data, axis=0)  # Add batch dimension

        input_name = self.yolo_session.get_inputs()[0].name
        outputs = self.yolo_session.run(None, {input_name: input_tensor})

        # L·∫•y output th√¥, output c·ªßa YOLOv8 th∆∞·ªùng c√≥ shape (1, 84, 8400) ho·∫∑c t∆∞∆°ng t·ª±
        # Ch√∫ng ta c·∫ßn transpose n√≥ th√†nh (1, 8400, 84) ƒë·ªÉ d·ªÖ x·ª≠ l√Ω
        predictions = np.squeeze(outputs[0]).T

        # L·ªçc c√°c box c√≥ ƒëi·ªÉm tin c·∫≠y (objectness score) th·∫•p
        # C·ªôt 4 trong predictions l√† ƒëi·ªÉm tin c·∫≠y t·ªïng th·ªÉ c·ªßa box
        scores = np.max(predictions[:, 4:], axis=1)
        predictions = predictions[scores > self.YOLO_CONF_THRESHOLD, :]
        scores = scores[scores > self.YOLO_CONF_THRESHOLD]

        if predictions.shape[0] == 0:
            rospy.loginfo("YOLO kh√¥ng ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng n√†o v∆∞·ª£t ng∆∞·ª°ng tin c·∫≠y.")
            return []

        # L·∫•y class_id c√≥ ƒëi·ªÉm cao nh·∫•t
        class_ids = np.argmax(predictions[:, 4:], axis=1)

        # L·∫•y t·ªça ƒë·ªô box v√† chuy·ªÉn ƒë·ªïi v·ªÅ ·∫£nh g·ªëc
        x, y, w, h = predictions[:, 0], predictions[:, 1], predictions[:, 2], predictions[:, 3]
        
        # T√≠nh to√°n t·ª∑ l·ªá scale ƒë·ªÉ chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô
        x_scale = original_width / self.YOLO_INPUT_SIZE[0]
        y_scale = original_height / self.YOLO_INPUT_SIZE[1]

        # Chuy·ªÉn t·ª´ [center_x, center_y, width, height] sang [x1, y1, x2, y2]
        x1 = (x - w / 2) * x_scale
        y1 = (y - h / 2) * y_scale
        x2 = (x + w / 2) * x_scale
        y2 = (y + h / 2) * y_scale
        
        # Chuy·ªÉn th√†nh list c√°c box v√† scores
        boxes = np.column_stack((x1, y1, x2, y2)).tolist()
        
        # 4. Th·ª±c hi·ªán Non-Maximum Suppression (NMS)
        # ƒê√¢y l√† m·ªôt b∆∞·ªõc c·ª±c k·ª≥ quan tr·ªçng ƒë·ªÉ lo·∫°i b·ªè c√°c box tr√πng l·∫∑p
        # OpenCV cung c·∫•p m·ªôt h√†m NMS hi·ªáu qu·∫£
        nms_threshold = 0.45 # Ng∆∞·ª°ng IOU ƒë·ªÉ lo·∫°i b·ªè box
        indices = self.numpy_nms(np.array(boxes), scores, nms_threshold)
        
        if len(indices) == 0:
            rospy.loginfo("YOLO: Sau NMS, kh√¥ng c√≤n ƒë·ªëi t∆∞·ª£ng n√†o.")
            return []

        # 5. T·∫°o danh s√°ch k·∫øt qu·∫£ cu·ªëi c√πng
        final_detections = []
        for i in indices.flatten():
            final_detections.append({
                'class_name': self.YOLO_CLASS_NAMES[class_ids[i]],
                'confidence': float(scores[i]),
                'box': [int(coord) for coord in boxes[i]] # Chuy·ªÉn t·ªça ƒë·ªô sang int
            })

        rospy.loginfo(f"YOLO ƒë√£ ph√°t hi·ªán {len(final_detections)} ƒë·ªëi t∆∞·ª£ng cu·ªëi c√πng.")
        return final_detections

    def initialize_mqtt(self):
        self.mqtt_client = mqtt.Client()
        def on_connect(client, userdata, flags, rc): rospy.loginfo(f"K·∫øt n·ªëi MQTT: {'Th√†nh c√¥ng' if rc == 0 else 'Th·∫•t b·∫°i'}")
        self.mqtt_client.on_connect = on_connect
        try:
            self.mqtt_client.connect(self.MQTT_BROKER, self.MQTT_PORT, 60)
            self.mqtt_client.loop_start()
        except Exception as e: rospy.logerr(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi MQTT: {e}")
    
    def _set_state(self, new_state, initial=False):
        if self.current_state != new_state:
            if not initial: rospy.loginfo(f"Chuy·ªÉn tr·∫°ng th√°i: {self.current_state.name if self.current_state else 'None'} -> {new_state.name}")
            
            # Reset line validation counter khi r·ªùi kh·ªèi LINE_VALIDATION state
            if self.current_state == RobotState.LINE_VALIDATION and new_state != RobotState.LINE_VALIDATION:
                self.line_validation_attempts = 0
                
            self.current_state = new_state
            self.state_change_time = rospy.get_time()

    def camera_callback(self, image_msg):
        try:
            if image_msg.encoding.endswith('compressed'):
                np_arr = np.frombuffer(image_msg.data, np.uint8)
                cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
            else:
                cv_image = np.frombuffer(image_msg.data, dtype=np.uint8).reshape(image_msg.height, image_msg.width, -1)
            if 'rgb' in image_msg.encoding: cv_image = cv2.cvtColor(cv_image, cv2.COLOR_RGB2BGR)
            self.latest_image = cv2.resize(cv_image, (self.WIDTH, self.HEIGHT))
        except Exception as e: rospy.logerr(f"L·ªói chuy·ªÉn ƒë·ªïi ·∫£nh: {e}")

    def run(self):
        rospy.loginfo("B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p. ƒê·ª£i 3 gi√¢y..."); time.sleep(3); rospy.loginfo("H√†nh tr√¨nh b·∫Øt ƒë·∫ßu!")
        self.detector.start_scanning()
        rate = rospy.Rate(20)
        while not rospy.is_shutdown():
            # ===================================================================
            # INITIAL FLAG CHECK: ƒê·ª£i red flag l·∫ßn ƒë·∫ßu ƒë·ªÉ b·∫Øt ƒë·∫ßu
            # ===================================================================
            if not self.robot_started:
                if self.latest_image is not None:
                    if self.detect_red_flag(self.latest_image):
                        if not self.initial_red_flag_detected:
                            rospy.loginfo("üö© PH√ÅT HI·ªÜN C·ªú ƒê·ªé L·∫¶N ƒê·∫¶U - ƒê√£ s·∫µn s√†ng ƒë·ªÉ b·∫Øt ƒë·∫ßu!")
                            self.initial_red_flag_detected = True
                        # V·∫´n ch·ªù cho ƒë·∫øn khi c·ªù ƒë∆∞·ª£c ph·∫•t (kh√¥ng c√≥ red flag)
                        rospy.loginfo("üö© C·ªù v·∫´n c√≤n - ƒë·ª£i ph·∫•t c·ªù ƒë·ªÉ b·∫Øt ƒë·∫ßu...")
                        self.robot.stop()
                        time.sleep(1.0)
                        continue
                    elif self.initial_red_flag_detected:
                        # Red flag ƒë√£ ƒë∆∞·ª£c detect l·∫ßn ƒë·∫ßu v√† gi·ªù kh√¥ng c√≤n n·ªØa - b·∫Øt ƒë·∫ßu!
                        rospy.loginfo("‚úÖ C·ªú ƒê√É ƒê∆Ø·ª¢C PH·∫§T - B·∫Øt ƒë·∫ßu ch·∫°y!")
                        self.robot_started = True
                    else:
                        # Ch∆∞a th·∫•y red flag l·∫ßn ƒë·∫ßu - ch·ªù
                        rospy.loginfo_throttle(3, "‚è≥ ƒê·ª£i ph√°t hi·ªán c·ªù ƒë·ªè l·∫ßn ƒë·∫ßu ƒë·ªÉ b·∫Øt ƒë·∫ßu...")
                        self.robot.stop()
                        rate.sleep()
                        continue
                else:
                    rospy.logwarn_throttle(5, "ƒêang ch·ªù d·ªØ li·ªáu camera...")
                    self.robot.stop()
                    rate.sleep()
                    continue
            
            # ===================================================================
            # RUNTIME FLAG CHECK: Ki·ªÉm tra c·ªù ƒë·ªè khi ƒëang ch·∫°y - reset t·ª´ ƒë·∫ßu n·∫øu th·∫•y
            # ===================================================================
            if self.robot_started and self.latest_image is not None and self.detect_red_flag(self.latest_image):
                rospy.loginfo("ÔøΩ PH√ÅT HI·ªÜN C·ªú ƒê·ªé KHI ƒêANG CH·∫†Y - RESET V·ªÄ TR·∫†NG TH√ÅI BAN ƒê·∫¶U!")
                # Reset to√†n b·ªô robot v·ªÅ tr·∫°ng th√°i ban ƒë·∫ßu
                self.reset_robot_to_initial_state()
                time.sleep(1.0)  # Short pause before restarting
                continue  # Restart from beginning of loop
                
            # ===================================================================
            # TR·∫†NG TH√ÅI 1: ƒêANG B√ÅM LINE (DRIVING_STRAIGHT)
            # ===================================================================
            if self.current_state == RobotState.DRIVING_STRAIGHT:
                if self.latest_image is None:
                    rospy.logwarn_throttle(5, "ƒêang ch·ªù d·ªØ li·ªáu h√¨nh ·∫£nh t·ª´ topic camera...")
                    self.robot.stop()
                    rate.sleep()
                    continue

                # --- B∆Ø·ªöC 1: KI·ªÇM TRA GIAO L·ªò V·ªöI CAMERA-LIDAR CONFIRMATION ---
                # Camera detect tr∆∞·ªõc, LiDAR confirm sau ƒë·ªÉ tr√°nh nhi·ªÖu
                if self.check_camera_lidar_intersection():
                    rospy.loginfo("S·ª∞ KI·ªÜN (Camera+LiDAR): X√°c nh·∫≠n giao l·ªô. D·ª´ng ngay l·∫≠p t·ª©c.")
                    self.robot.stop()
                    time.sleep(0.5) # Ch·ªù robot d·ª´ng h·∫≥n

                    # C·∫≠p nh·∫≠t v·ªã tr√≠ hi·ªán t·∫°i (ƒë√£ ƒë·∫øn ƒë√≠ch) v√† x·ª≠ l√Ω
                    self.current_node_id = self.target_node_id
                    rospy.loginfo(f"==> ƒê√É ƒê·∫æN node {self.current_node_id}.")

                    if self.current_node_id == self.navigator.end_node:
                        rospy.loginfo("ƒê√É ƒê·∫æN ƒê√çCH CU·ªêI C√ôNG!")
                        self._set_state(RobotState.GOAL_REACHED)
                    else:
                        self._set_state(RobotState.HANDLING_EVENT)
                        self.handle_intersection()
                    continue # B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p m·ªõi v·ªõi tr·∫°ng th√°i m·ªõi

                # --- B∆Ø·ªöC 2: LOGIC "NH√åN XA H∆†N" V·ªöI ROI D·ª∞ B√ÅO ---
                # N·∫øu LiDAR im l·∫∑ng, ki·ªÉm tra xem v·∫°ch k·∫ª c√≥ s·∫Øp bi·∫øn m·∫•t ·ªü ph√≠a xa kh√¥ng.
                lookahead_line_center = self._get_line_center(self.latest_image, self.LOOKAHEAD_ROI_Y, self.LOOKAHEAD_ROI_H)

                if lookahead_line_center is None:
                    rospy.logwarn("S·ª∞ KI·ªÜN (D·ª± b√°o): V·∫°ch k·∫ª ƒë∆∞·ªùng bi·∫øn m·∫•t ·ªü ph√≠a xa. Chu·∫©n b·ªã v√†o giao l·ªô.")
                    # H√†nh ƒë·ªông ph√≤ng ng·ª´a: chuy·ªÉn sang tr·∫°ng th√°i ƒëi th·∫≥ng v√†o giao l·ªô.
                    self._set_state(RobotState.APPROACHING_INTERSECTION)
                    time.sleep(0.3)
                    continue # B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p m·ªõi v·ªõi tr·∫°ng th√°i m·ªõi

                # --- B∆Ø·ªöC 3: B√ÅM LINE B√åNH TH∆Ø·ªúNG (N·∫æU PH√çA TR∆Ø·ªöC AN TO√ÄN) ---
                # Ch·ªâ khi c·∫£ LiDAR v√† ROI D·ª± b√°o ƒë·ªÅu ·ªïn, ta m·ªõi th·ª±c hi·ªán b√°m line.
                execution_line_center = self._get_line_center(self.latest_image, self.ROI_Y, self.ROI_H)

                if execution_line_center is not None:
                    # Ki·ªÉm tra xem line c√≥ n·∫±m trong kho·∫£ng h·ª£p l·ªá kh√¥ng tr∆∞·ªõc khi b√°m
                    if not self._is_line_in_valid_range(self.latest_image):
                        rospy.logwarn("S·ª∞ KI·ªÜN: Line position kh√¥ng h·ª£p l·ªá, chuy·ªÉn sang LINE_VALIDATION ƒë·ªÉ ki·ªÉm tra.")
                        self.line_validation_attempts = 0  # Reset counter
                        self._set_state(RobotState.LINE_VALIDATION)
                        continue
                    
                    # An to√†n ƒë·ªÉ b√°m line, v√¨ ch√∫ng ta bi·∫øt ph√≠a tr∆∞·ªõc kh√¥ng c√≥ giao l·ªô ƒë·ªôt ng·ªôt.
                    self.correct_course(execution_line_center)
                    
                    # Ph√¢n t√≠ch v√† in g√≥c line (n·∫øu ƒë∆∞·ª£c b·∫≠t)
                else:
                    # Tr∆∞·ªùng h·ª£p hi·∫øm: ROI xa th·∫•y line nh∆∞ng ROI g·∫ßn l·∫°i kh√¥ng. D·ª´ng l·∫°i cho an to√†n.
                    rospy.logwarn("Tr·∫°ng th√°i kh√¥ng nh·∫•t qu√°n: ROI xa th·∫•y line, ROI g·∫ßn kh√¥ng th·∫•y. T·∫°m d·ª´ng an to√†n.")
                    self.robot.stop()

            # ===================================================================
            # TR·∫†NG TH√ÅI 1.5: KI·ªÇM TRA V√Ä X√ÅC TH·ª∞C V·ªä TR√ç LINE (LINE_VALIDATION)
            # ===================================================================
            elif self.current_state == RobotState.LINE_VALIDATION:
                if self.latest_image is None:
                    rospy.logwarn("LINE_VALIDATION: Ch·ªù d·ªØ li·ªáu camera...")
                    self.robot.stop()
                    rate.sleep()
                    continue

                # Ki·ªÉm tra line c√≥ n·∫±m trong kho·∫£ng h·ª£p l·ªá kh√¥ng
                if self._is_line_in_valid_range(self.latest_image):
                    rospy.loginfo("LINE_VALIDATION: Line position h·ª£p l·ªá, ti·∫øp t·ª•c b√°m line.")
                    self._set_state(RobotState.DRIVING_STRAIGHT)
                    continue
                else:
                    # Line kh√¥ng h·ª£p l·ªá, th·ª≠ ƒëi·ªÅu ch·ªânh
                    self.line_validation_attempts += 1
                    rospy.logwarn(f"LINE_VALIDATION: Line kh√¥ng h·ª£p l·ªá, l·∫ßn th·ª≠ {self.line_validation_attempts}/{self.LINE_VALIDATION_ATTEMPTS}")
                    
                    if self.line_validation_attempts >= self.LINE_VALIDATION_ATTEMPTS:
                        rospy.logerr("LINE_VALIDATION: ƒê√£ th·ª≠ t·ªëi ƒëa, chuy·ªÉn sang t√¨m ki·∫øm line m·ªõi.")
                        self._set_state(RobotState.REACQUIRING_LINE)
                        continue
                    
                    # Th·ª≠ ƒëi·ªÅu ch·ªânh nh·∫π ƒë·ªÉ t√¨m l·∫°i line
                    line_center = self._get_line_center(self.latest_image, self.ROI_Y, self.ROI_H)
                    if line_center is not None:
                        error = line_center - (self.WIDTH / 2)
                        if abs(error) > 0:
                            # ƒêi·ªÅu ch·ªânh nh·∫π v·ªÅ ph√≠a line
                            adj = np.clip(error / (self.WIDTH / 2) * 0.3, -0.1, 0.1)
                            self.robot.set_motors(self.BASE_SPEED * 0.5 + adj, self.BASE_SPEED * 0.5 - adj)
                        else:
                            self.robot.set_motors(self.BASE_SPEED * 0.5, self.BASE_SPEED * 0.5)
                    else:
                        self.robot.stop()
                
                # Timeout check
                if rospy.get_time() - self.state_change_time > self.LINE_VALIDATION_TIMEOUT:
                    rospy.logwarn("LINE_VALIDATION: Timeout, chuy·ªÉn sang t√¨m ki·∫øm line m·ªõi.")
                    self._set_state(RobotState.REACQUIRING_LINE)

            # ===================================================================
            # TR·∫†NG TH√ÅI 2: ƒêANG TI·∫æN V√ÄO GIAO L·ªò (APPROACHING_INTERSECTION)
            # ===================================================================
            elif self.current_state == RobotState.APPROACHING_INTERSECTION:
                # ƒêi th·∫≥ng m·ªôt ƒëo·∫°n ng·∫Øn ƒë·ªÉ v√†o trung t√¢m giao l·ªô
                self.robot.set_motors(self.BASE_SPEED, self.BASE_SPEED)
                
                if rospy.get_time() - self.state_change_time > self.INTERSECTION_APPROACH_DURATION:
                    rospy.loginfo("ƒê√£ ti·∫øn v√†o trung t√¢m giao l·ªô. D·ª´ng l·∫°i ƒë·ªÉ x·ª≠ l√Ω.")
                    self.robot.stop(); time.sleep(0.5)

                    self.current_node_id = self.target_node_id
                    rospy.loginfo(f"==> ƒê√É ƒê·∫æN node {self.current_node_id}.")

                    if self.current_node_id == self.navigator.end_node:
                        rospy.loginfo("ƒê√É ƒê·∫æN ƒê√çCH CU·ªêI C√ôNG!")
                        self._set_state(RobotState.GOAL_REACHED)
                    else:
                        self._set_state(RobotState.HANDLING_EVENT)
                        self.handle_intersection()

            # ===================================================================
            # TR·∫†NG TH√ÅI 3: ƒêANG R·ªúI KH·ªéI GIAO L·ªò (LEAVING_INTERSECTION)
            # ===================================================================
            elif self.current_state == RobotState.LEAVING_INTERSECTION:
                self.robot.set_motors(self.BASE_SPEED, self.BASE_SPEED)
                if rospy.get_time() - self.state_change_time > self.INTERSECTION_CLEARANCE_DURATION:
                    rospy.loginfo("ƒê√£ tho√°t kh·ªèi khu v·ª±c giao l·ªô. B·∫Øt ƒë·∫ßu t√¨m ki·∫øm line m·ªõi.")
                    self._set_state(RobotState.REACQUIRING_LINE)
            
            # ===================================================================
            # TR·∫†NG TH√ÅI 4: ƒêANG T√åM L·∫†I LINE (REACQUIRING_LINE)
            # ===================================================================
            elif self.current_state == RobotState.REACQUIRING_LINE:
                self.robot.set_motors(self.BASE_SPEED, self.BASE_SPEED)
                line_center_x = self._get_line_center(self.latest_image, self.ROI_Y, self.ROI_H)
                
                if line_center_x is not None:
                    rospy.loginfo("ƒê√£ t√¨m th·∫•y line m·ªõi! Chuy·ªÉn sang ch·∫ø ƒë·ªô b√°m line.")
                    self._set_state(RobotState.DRIVING_STRAIGHT)
                    continue
                
                if rospy.get_time() - self.state_change_time > self.LINE_REACQUIRE_TIMEOUT:
                    rospy.logerr("Kh√¥ng th·ªÉ t√¨m th·∫•y line m·ªõi sau khi r·ªùi giao l·ªô. D·ª´ng l·∫°i.")
                    self._set_state(RobotState.DEAD_END)

            # ===================================================================
            # TR·∫†NG TH√ÅI K·∫æT TH√öC (DEAD_END, GOAL_REACHED)
            # ===================================================================
            elif self.current_state == RobotState.DEAD_END:
                rospy.logwarn("ƒê√£ v√†o ng√µ c·ª•t ho·∫∑c g·∫∑p l·ªói kh√¥ng th·ªÉ ph·ª•c h·ªìi. D·ª´ng ho·∫°t ƒë·ªông."); self.robot.stop(); break
            elif self.current_state == RobotState.GOAL_REACHED: 
                rospy.loginfo("ƒê√É HO√ÄN TH√ÄNH NHI·ªÜM V·ª§. D·ª´ng ho·∫°t ƒë·ªông."); self.robot.stop(); break

            if self.video_writer is not None and self.latest_image is not None:
                # L·∫•y ·∫£nh g·ªëc, v·∫Ω th√¥ng tin l√™n, r·ªìi ghi
                debug_frame = self.draw_debug_info(self.latest_image)
                if debug_frame is not None:
                    self.video_writer.write(debug_frame)

            rate.sleep()
        self.cleanup()

    def cleanup(self):
        rospy.loginfo("D·ª´ng robot v√† gi·∫£i ph√≥ng t√†i nguy√™n..."); self.robot.stop()
        self.detector.stop_scanning(); self.mqtt_client.loop_stop(); self.mqtt_client.disconnect()
        rospy.loginfo("ƒê√£ gi·∫£i ph√≥ng t√†i nguy√™n. Ch∆∞∆°ng tr√¨nh k·∫øt th√∫c.")

    def map_absolute_to_relative(self, target_direction_label, current_robot_direction):
        """
        Chuy·ªÉn ƒë·ªïi h∆∞·ªõng tuy·ªát ƒë·ªëi ('N', 'E', 'S', 'W') th√†nh h√†nh ƒë·ªông t∆∞∆°ng ƒë·ªëi ('straight', 'left', 'right').
        V√≠ d·ª•: robot ƒëang h∆∞·ªõng B·∫ÆC (NORTH), m·ª•c ti√™u l√† ƒëi h∆∞·ªõng ƒê√îNG (EAST) -> h√†nh ƒë·ªông l√† 'right'.
        """
        target_dir = self.LABEL_TO_DIRECTION_ENUM.get(target_direction_label)
        if target_dir is None: return None

        current_idx = current_robot_direction.value
        target_idx = target_dir.value
        
        diff = (target_idx - current_idx + 4) % 4 
        
        if diff == 0:
            return 'straight'
        elif diff == 1:
            return 'right'
        elif diff == 3: 
            return 'left'
        else: 
            return 'turn_around'
        
    def map_relative_to_absolute(self, relative_action, current_robot_direction):
        """
        Chuy·ªÉn ƒë·ªïi h√†nh ƒë·ªông t∆∞∆°ng ƒë·ªëi ('straight', 'left', 'right') th√†nh h∆∞·ªõng tuy·ªát ƒë·ªëi ('N', 'E', 'S', 'W').
        """
        current_idx = current_robot_direction.value
        if relative_action == 'straight':
            target_idx = current_idx
        elif relative_action == 'right':
            target_idx = (current_idx + 1) % 4
        elif relative_action == 'left':
            target_idx = (current_idx - 1 + 4) % 4
        else:
            return None
        
        for label, direction in self.LABEL_TO_DIRECTION_ENUM.items():
            if direction.value == target_idx:
                return label
        return None
    
    def _get_line_center(self, image, roi_y, roi_h):
        """Ki·ªÉm tra s·ª± t·ªìn t·∫°i v√† v·ªã tr√≠ c·ªßa v·∫°ch k·∫ª trong m·ªôt ROI c·ª• th·ªÉ v·ªõi improved detection."""
        if image is None: return None
        roi = image[roi_y : roi_y + roi_h, :]
        
        # === B∆Ø·ªöC 1: HSV COLOR-BASED DETECTION ===
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        color_mask = cv2.inRange(hsv, self.LINE_COLOR_LOWER, self.LINE_COLOR_UPPER)
        
        # === B∆Ø·ªöC 2: GRAYSCALE THRESHOLD DETECTION (BACKUP METHOD) ===
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        # Use THRESH_BINARY_INV to get white pixels for dark lines
        _, thresh_mask = cv2.threshold(gray, 60, 255, cv2.THRESH_BINARY_INV)
        
        # === B∆Ø·ªöC 3: COMBINE BOTH METHODS ===
        # Use logical OR to combine both detection methods
        combined_mask = cv2.bitwise_or(color_mask, thresh_mask)
        
        # === B∆Ø·ªöC 4: T·∫†O M·∫∂T N·∫† T·∫¨P TRUNG (FOCUS MASK) ===
        focus_mask = np.zeros_like(combined_mask)
        roi_height, roi_width = focus_mask.shape
        
        center_width = int(roi_width * self.ROI_CENTER_WIDTH_PERCENT)
        start_x = (roi_width - center_width) // 2
        end_x = start_x + center_width
        
        # V·∫Ω m·ªôt h√¨nh ch·ªØ nh·∫≠t tr·∫Øng ·ªü gi·ªØa
        cv2.rectangle(focus_mask, (start_x, 0), (end_x, roi_height), 255, -1)
        
        # === B∆Ø·ªöC 5: K·∫æT H·ª¢P DETECTION V√Ä FOCUS MASK ===
        # Ch·ªâ gi·ªØ l·∫°i nh·ªØng pixel tr·∫Øng n√†o xu·∫•t hi·ªán ·ªü c·∫£ detection mask v√† focus mask
        final_mask = cv2.bitwise_and(combined_mask, focus_mask)

        # (T√πy ch·ªçn) Hi·ªÉn th·ªã mask ƒë·ªÉ debug
        # cv2.imshow("HSV Mask", color_mask)
        # cv2.imshow("Grayscale Mask", thresh_mask)
        # cv2.imshow("Combined Mask", combined_mask)
        # cv2.imshow("Focus Mask", focus_mask)
        # cv2.imshow("Final Mask", final_mask)
        # cv2.waitKey(1)

        # T√¨m contours tr√™n m·∫∑t n·∫° cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l·ªçc
        _, contours, _ = cv2.findContours(final_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        if not contours:
            return None
            
        c = max(contours, key=cv2.contourArea)
        
        if cv2.contourArea(c) < self.SCAN_PIXEL_THRESHOLD:
            return None

        M = cv2.moments(c)
        if M["m00"] > 0:
            # Quan tr·ªçng: Tr·ªçng t√¢m b√¢y gi·ªù ƒë∆∞·ª£c t√≠nh to√°n ch·ªâ d·ª±a tr√™n v·∫°ch k·∫ª trong khu v·ª±c trung t√¢m
            return int(M["m10"] / M["m00"])
        return None
    
    def _is_line_in_valid_range(self, image):
        """
        Ki·ªÉm tra xem v·∫°ch k·∫ª c√≥ n·∫±m trong kho·∫£ng h·ª£p l·ªá kh√¥ng.
        Returns: 
            - True n·∫øu line n·∫±m trong kho·∫£ng cho ph√©p
            - False n·∫øu line qu√° l·ªách ho·∫∑c kh√¥ng t√¨m th·∫•y
        """
        if image is None:
            return False
            
        line_center = self._get_line_center(image, self.ROI_Y, self.ROI_H)
        if line_center is None:
            return False
            
        image_center = self.WIDTH / 2
        max_deviation = self.WIDTH * self.LINE_CENTER_TOLERANCE
        deviation = abs(line_center - image_center)
        
        is_valid = deviation <= max_deviation
        
        rospy.loginfo(f"Line validation: center={line_center}, deviation={deviation:.1f}, max_allowed={max_deviation:.1f}, valid={is_valid}")
        return is_valid
    
    def calculate_line_angle_from_camera(self):
        """
        T√≠nh g√≥c c·ªßa line t·ª´ camera d·ª±a tr√™n v·ªã tr√≠ tr·ªçng t√¢m.
        Returns: (angle_degrees, confidence)
        """
        line_center = self._get_line_center(self.latest_image, self.ROI_Y, self.ROI_H)
        if line_center is None:
            return None, "NO_LINE_DETECTED"
        
        image_center = self.WIDTH / 2
        pixel_offset = line_center - image_center
        
        # Chuy·ªÉn ƒë·ªïi pixel offset th√†nh g√≥c (s·ª≠ d·ª•ng FOV 60 degrees)
        camera_fov = 60
        angle = (pixel_offset / image_center) * (camera_fov / 2)
        
        # ƒê√°nh gi√° confidence
        if abs(angle) < 3:
            confidence = "HIGH"
        elif abs(angle) < 10:
            confidence = "MEDIUM"
        else:
            confidence = "LOW"
        
        return angle, confidence
    
    def find_line_clusters_in_lidar(self):
        """
        T√¨m c√°c clusters c√≥ th·ªÉ l√† line trong d·ªØ li·ªáu LiDAR.
        """
        if self.detector.latest_scan is None:
            return []
        
        scan = self.detector.latest_scan
        ranges = np.array(scan.ranges)
        n = len(ranges)
        
        angle_increment_deg = math.degrees(scan.angle_increment)
        points_per_range = int(self.detector.angle_range / angle_increment_deg)
        
        line_clusters = []
        front_angle_range = 45  # ¬±45 degrees
        
        # Qu√©t theo t·ª´ng zone ·ªü ph√≠a tr∆∞·ªõc
        for start_idx in range(0, n, points_per_range // 2):
            end_idx = min(start_idx + points_per_range, n)
            if end_idx - start_idx < points_per_range // 2:
                continue
            
            center_idx = start_idx + (end_idx - start_idx) // 2
            center_angle = self.detector.index_to_angle(center_idx, scan)
            
            # Ch·ªâ x√©t c√°c zone ·ªü ph√≠a tr∆∞·ªõc
            if abs(center_angle) <= front_angle_range:
                cluster = self.detect_line_cluster_in_zone(ranges[start_idx:end_idx], center_angle)
                if cluster:
                    line_clusters.append(cluster)
        
        return line_clusters
    
    def detect_line_cluster_in_zone(self, zone_ranges, center_angle):
        """
        Ph√°t hi·ªán line cluster trong m·ªôt zone.
        """
        if len(zone_ranges) == 0:
            return None
        
        # L·ªçc c√°c ƒëi·ªÉm h·ª£p l·ªá (tƒÉng range ƒë·ªÉ detect line xa h∆°n)
        min_dist, max_dist = 0.15, 2.0
        valid_mask = ((zone_ranges >= min_dist) & 
                     (zone_ranges <= max_dist) & 
                     np.isfinite(zone_ranges))
        
        if np.sum(valid_mask) < 5:  # √çt nh·∫•t 5 ƒëi·ªÉm
            return None
        
        valid_ranges = zone_ranges[valid_mask]
        
        # Ki·ªÉm tra ƒë·∫∑c ƒëi·ªÉm line: kho·∫£ng c√°ch t∆∞∆°ng ƒë·ªëi ƒë·ªìng ƒë·ªÅu
        distance_variance = np.var(valid_ranges)
        
        if distance_variance <= 0.4:  # Line c√≥ variance th·∫•p
            return {
                'center_angle': center_angle,
                'distance': np.mean(valid_ranges),
                'point_count': len(valid_ranges),
                'variance': distance_variance
            }
        
        return None
    
   
    def detect_camera_intersection(self):
        """
        Enhanced intersection detection with improved sensitivity for horizontal black lines.
        Uses multiple detection methods and morphological operations.
        """
        if self.latest_image is None:
            return False, "NO_IMAGE", None
        
        # L·∫•y ROI ƒë·ªÉ t√¨m ƒë∆∞·ªùng ngang (v·ªõi improved size)
        cross_detection_roi_y = int(self.HEIGHT * self.CROSS_DETECTION_ROI_Y_PERCENT)
        cross_detection_roi_h = int(self.HEIGHT * self.CROSS_DETECTION_ROI_H_PERCENT)
        
        roi = self.latest_image[cross_detection_roi_y:cross_detection_roi_y + cross_detection_roi_h, :]
        
        # === IMPROVED MULTI-METHOD DETECTION ===
        
        # Method 1: HSV-based detection
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        hsv_mask = cv2.inRange(hsv, self.LINE_COLOR_LOWER, self.LINE_COLOR_UPPER)
        
        # Method 2: Grayscale threshold detection
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        _, thresh_mask = cv2.threshold(gray, 60, 255, cv2.THRESH_BINARY_INV)
        
        # Method 3: Adaptive threshold for varying lighting
        adaptive_mask = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, 
                                            cv2.THRESH_BINARY_INV, 11, 2)
        
        # Combine all detection methods
        combined_mask = cv2.bitwise_or(hsv_mask, thresh_mask)
        combined_mask = cv2.bitwise_or(combined_mask, adaptive_mask)
        
        # === ENHANCED MORPHOLOGICAL OPERATIONS ===
        
        # Multi-scale morphological operations to handle different line thicknesses
        kernels = [
            np.ones((2,2), np.uint8),  # Small kernel for thin lines
            np.ones((3,3), np.uint8),  # Medium kernel
            np.ones((4,4), np.uint8)   # Larger kernel for thick lines
        ]
        
        processed_masks = []
        for kernel in kernels:
            # Close small gaps in lines
            closed = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
            # Remove noise
            opened = cv2.morphologyEx(closed, cv2.MORPH_OPEN, kernel)
            processed_masks.append(opened)
        
        # Combine results from different scales
        line_mask = processed_masks[0]
        for mask in processed_masks[1:]:
            line_mask = cv2.bitwise_or(line_mask, mask)
        
        # Final cleanup with horizontal-focused morphological operations
        horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 3))  # Favor horizontal structures
        line_mask = cv2.morphologyEx(line_mask, cv2.MORPH_CLOSE, horizontal_kernel)
        
        # T√¨m contours
        _, contours, _ = cv2.findContours(line_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return False, "NO_CONTOURS", None
        
        # Ki·ªÉm tra main line
        main_line_center = self._get_line_center(self.latest_image, self.ROI_Y, self.ROI_H)
        if main_line_center is None:
            return False, "NO_MAIN_LINE", None
        
        # === ENHANCED CROSS CANDIDATE ANALYSIS ===
        roi_height, roi_width = line_mask.shape
        cross_candidates = []
        
        for contour in contours:
            # Basic bounding rectangle
            x, y, w, h = cv2.boundingRect(contour)
            aspect_ratio = w / h if h > 0 else 0
            width_ratio = w / roi_width
            height_ratio = h / roi_height
            area = cv2.contourArea(contour)
            
            # More relaxed criteria for better detection
            is_horizontal = aspect_ratio > self.CROSS_MIN_ASPECT_RATIO
            is_wide_enough = width_ratio > self.CROSS_MIN_WIDTH_RATIO
            is_not_too_tall = height_ratio < self.CROSS_MAX_HEIGHT_RATIO
            is_big_enough = area > 30  # Reduced from 50 to 30 - catch smaller cross lines
            
            if is_horizontal and is_wide_enough and is_not_too_tall and is_big_enough:
                # Calculate more detailed properties
                M = cv2.moments(contour)
                if M["m00"] > 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])
                    
                    # Additional validation: check if it's roughly horizontal
                    # Fit line to contour points
                    if len(contour) >= 5:  # Need at least 5 points for fitting
                        [vx, vy, x0, y0] = cv2.fitLine(contour, cv2.DIST_L2, 0, 0.01, 0.01)
                        line_angle = np.arctan2(vy, vx) * 180 / np.pi
                        
                        # Check if line is roughly horizontal (within ¬±30 degrees)
                        if abs(line_angle) < 30 or abs(abs(line_angle) - 180) < 30:
                            cross_candidates.append({
                                'contour': contour,
                                'center_x': cx,
                                'center_y': cy,
                                'width': w,
                                'height': h,
                                'aspect_ratio': aspect_ratio,
                                'area': area,
                                'width_ratio': width_ratio,
                                'line_angle': line_angle
                            })
                    else:
                        # Fallback for small contours
                        cross_candidates.append({
                            'contour': contour,
                            'center_x': cx,
                            'center_y': cy,
                            'width': w,
                            'height': h,
                            'aspect_ratio': aspect_ratio,
                            'area': area,
                            'width_ratio': width_ratio,
                            'line_angle': 0
                        })
        
        if not cross_candidates:
            return False, "NO_CROSS_CANDIDATES", None
        
        # === IMPROVED CANDIDATE SCORING ===
        best_candidate = None
        best_score = 0
        
        for candidate in cross_candidates:
            # Multi-factor scoring
            area_score = min(candidate['area'] / (roi_width * roi_height), 1.0)  # Normalize and cap
            center_score = 1.0 - abs(candidate['center_x'] - roi_width/2) / (roi_width/2)
            aspect_score = min(candidate['aspect_ratio'] / 5.0, 1.0)  # Reward higher aspect ratios
            width_score = min(candidate['width_ratio'] / 0.8, 1.0)  # Reward wider lines
            
            # Weighted combination
            total_score = (area_score * 0.3 + 
                          center_score * 0.3 + 
                          aspect_score * 0.2 + 
                          width_score * 0.2)
            
            if total_score > best_score:
                best_score = total_score
                best_candidate = candidate
        
        if best_candidate is None:
            return False, "NO_GOOD_CANDIDATE", None
        
        # === ENHANCED CONFIDENCE ASSESSMENT ===
        confidence_level = "LOW"
        if best_score > 0.55:  # Reduced from 0.6 to 0.55
            confidence_level = "HIGH"
        elif best_score > 0.25:  # Reduced from 0.35 to 0.25 for better detection
            confidence_level = "MEDIUM"
        
        # Additional confidence boost for very horizontal lines
        if abs(best_candidate.get('line_angle', 0)) < 15:
            if confidence_level == "MEDIUM":
                confidence_level = "HIGH"
            elif confidence_level == "LOW":
                confidence_level = "MEDIUM"
        
        cross_line_center = best_candidate['center_x']
        return True, confidence_level, cross_line_center
    
    def detect_red_flag(self, image):
        """
        Detect red flag covering camera with improved red detection
        Returns True if red flag is detected, False otherwise
        """
        if image is None:
            return False
            
        # Convert BGR to HSV
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        
        # Create masks for both red ranges (red color wraps around in HSV)
        mask1 = cv2.inRange(hsv, self.FLAG_RED_LOWER1, self.FLAG_RED_UPPER1)  # Lower red range
        mask2 = cv2.inRange(hsv, self.FLAG_RED_LOWER2, self.FLAG_RED_UPPER2)  # Upper red range
        
        # Combine both masks
        red_mask = cv2.bitwise_or(mask1, mask2)
        
        # Apply morphological operations to clean up the mask
        kernel = np.ones((3,3), np.uint8)
        red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_CLOSE, kernel)  # Fill small gaps
        red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_OPEN, kernel)   # Remove noise
        
        # Calculate coverage percentage
        total_pixels = image.shape[0] * image.shape[1]
        red_pixels = cv2.countNonZero(red_mask)
        coverage_ratio = red_pixels / total_pixels
        
        # Debug log (can be removed later)
        if coverage_ratio > 0.05:  # Log if there's any significant red detected
            rospy.loginfo(f"üö© Red detection: {coverage_ratio:.3f} coverage (threshold: {self.FLAG_COVERAGE_THRESHOLD})")
        
        # Return True if coverage exceeds threshold
        return coverage_ratio > self.FLAG_COVERAGE_THRESHOLD
    
    def test_flag_detection_visual(self, image, show_debug=True):
        """
        Test flag detection with visual feedback for debugging
        """
        if image is None:
            return False, 0.0
            
        # Convert BGR to HSV
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        
        # Create masks for both red ranges
        mask1 = cv2.inRange(hsv, self.FLAG_RED_LOWER1, self.FLAG_RED_UPPER1)
        mask2 = cv2.inRange(hsv, self.FLAG_RED_LOWER2, self.FLAG_RED_UPPER2)
        red_mask = cv2.bitwise_or(mask1, mask2)
        
        # Apply morphological operations
        kernel = np.ones((3,3), np.uint8)
        red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_CLOSE, kernel)
        red_mask = cv2.morphologyEx(red_mask, cv2.MORPH_OPEN, kernel)
        
        # Calculate coverage
        total_pixels = image.shape[0] * image.shape[1]
        red_pixels = cv2.countNonZero(red_mask)
        coverage_ratio = red_pixels / total_pixels
        
        if show_debug:
            # Show debug windows (comment out in production)
            cv2.imshow("Original", image)
            cv2.imshow("Red Mask", red_mask)
            cv2.imshow("HSV", hsv)
            cv2.waitKey(1)
            rospy.loginfo(f"Flag detection test: Coverage={coverage_ratio:.4f}, Threshold={self.FLAG_COVERAGE_THRESHOLD}")
        
        return coverage_ratio > self.FLAG_COVERAGE_THRESHOLD, coverage_ratio
    
    def check_camera_lidar_intersection(self):
        """
        Ki·ªÉm tra giao l·ªô v·ªõi logic: Camera detect tr∆∞·ªõc, LiDAR confirm sau.
        Returns: True if intersection confirmed by both sensors
        """
        current_time = rospy.get_time()
        
        # B∆∞·ªõc 1: Camera detection
        if not self.camera_intersection_detected and not self.waiting_for_lidar_confirmation:
            camera_detected, camera_conf, cross_center = self.detect_camera_intersection()
            
            if camera_detected: # Khi detect ƒë∆∞·ª£c giao l·ªô b·∫±ng camera di chuy·ªÉn th√™m 1 ƒëo·∫°n ƒë·ªÉ tr√°nh nhi·ªÖu
                rospy.loginfo(f"üì∑ CAMERA: Intersection detected! Confidence: {camera_conf}")
                rospy.loginfo(f"üì∑ Cross line center: {cross_center}, Main line center: {self._get_line_center(self.latest_image, self.ROI_Y, self.ROI_H)}")
                print(f"üì∑ Cross line center: {cross_center}, Main line center: {self._get_line_center(self.latest_image, self.ROI_Y, self.ROI_H)}")
                # Ch·ªâ trigger n·∫øu confidence ƒë·ªß cao
                if camera_conf in ["HIGH", "MEDIUM"]:
                    self.camera_intersection_detected = True
                    self.camera_detection_time = current_time
                    
                    # Di chuy·ªÉn th√™m m·ªôt ƒëo·∫°n ƒë·ªÉ ƒë·∫£m b·∫£o LiDAR v√†o ƒë√∫ng v·ªã tr√≠
                    rospy.loginfo("üöÄ Moving forward to position LiDAR for better detection...")
                    self.move_forward_briefly()
                    
                    self.waiting_for_lidar_confirmation = True
                    rospy.loginfo("üì∑ CAMERA: Waiting for LiDAR confirmation...")
                    return False  # Ch∆∞a confirm, ch·ªâ m·ªõi detect
        
        # B∆∞·ªõc 2: Ch·ªù LiDAR confirmation
        if self.waiting_for_lidar_confirmation:
            # Ki·ªÉm tra timeout
            if current_time - self.camera_detection_time > self.lidar_confirmation_timeout:
                rospy.logwarn("‚è∞ TIMEOUT: LiDAR didn't confirm intersection, resetting camera detection")
                self.reset_intersection_detection()
                return False
            
            # Ki·ªÉm tra LiDAR confirmation
            if self.detector.process_detection():
                rospy.loginfo("‚úÖ INTERSECTION CONFIRMED: Both camera and LiDAR detected intersection!")
                rospy.loginfo("üì° LiDAR confirmed camera's intersection detection")
                rospy.loginfo("üõë IMMEDIATE STOP: Robot stopping due to intersection confirmation")
                
                # D·ª™NG NGAY L·∫¨P T·ª®C khi LiDAR confirm
                if hasattr(self, 'robot') and self.robot:
                    self.robot.stop()
                
                # Reset flags
                self.reset_intersection_detection()
                return True  # Intersection confirmed!
            else:
                # V·∫´n ch·ªù LiDAR confirmation
                elapsed = current_time - self.camera_detection_time
                rospy.loginfo(f"‚è≥ Waiting for LiDAR confirmation... ({elapsed:.1f}s/{self.lidar_confirmation_timeout}s)")
                return False
        
        return False
    
    def reset_intersection_detection(self):
        """Reset tr·∫°ng th√°i detection."""
        self.camera_intersection_detected = False
        self.camera_detection_time = 0
        self.waiting_for_lidar_confirmation = False
    
    def reset_robot_to_initial_state(self):
        """Reset robot v·ªÅ tr·∫°ng th√°i ban ƒë·∫ßu ƒë·ªÉ ch·∫°y l·∫°i t·ª´ ƒë·∫ßu."""
        rospy.loginfo("üîÑ RESETTING ROBOT TO INITIAL STATE...")
        
        # Stop robot immediately
        self.robot.stop()
        
        # Reset flag tracking
        self.initial_red_flag_detected = False
        self.robot_started = False
        
        # Reset intersection detection states
        self.reset_intersection_detection()
        
        # Reset line validation
        self.line_validation_attempts = 0
        
        # Reset to initial driving state
        self._set_state(RobotState.DRIVING_STRAIGHT, initial=True)
        
        rospy.loginfo("‚ú® Robot reset completed - ready to wait for initial red flag!")
    
    def move_forward_briefly(self):
        """
        Di chuy·ªÉn robot th·∫≥ng m·ªôt ƒëo·∫°n ng·∫Øn ƒë·ªÉ ƒë·∫£m b·∫£o LiDAR v√†o ƒë√∫ng v·ªã tr√≠ detection.
        """
        rospy.loginfo("üöÄ Moving forward briefly for better LiDAR positioning...")
        
        # Di chuy·ªÉn th·∫≥ng v·ªõi speed th·∫•p h∆°n base speed ƒë·ªÉ an to√†n
        forward_speed = self.BASE_SPEED * 0.7
        self.robot.set_motors(forward_speed, forward_speed)

        # Di chuy·ªÉn trong 1.5 gi√¢y (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)
        time.sleep(1.5)
        
        # D·ª´ng l·∫°i
        self.robot.stop()
        rospy.loginfo("‚úÖ Forward movement completed, ready for LiDAR detection")
    
    def correct_course(self, line_center_x):
        """
        H√†m b√°m line an to√†n v·ªõi c∆° ch·∫ø gi·ªõi h·∫°n l·ª±c b·∫ª l√°i.
        """
        error = line_center_x - (self.WIDTH / 2)
        
        # V·∫´n ƒëi th·∫≥ng n·∫øu sai s·ªë r·∫•t nh·ªè
        if abs(error) < (self.WIDTH / 2) * self.SAFE_ZONE_PERCENT:
            self.robot.set_motors(self.BASE_SPEED, self.BASE_SPEED)
            return

        # T√≠nh to√°n l·ª±c ƒëi·ªÅu ch·ªânh
        adj = (error / (self.WIDTH / 2)) * self.CORRECTION_GAIN

        # NgƒÉn ch·∫∑n h√†nh vi b·∫ª l√°i qu√° g·∫Øt m·ªôt c√°ch tuy·ªát ƒë·ªëi
        adj = np.clip(adj, -self.MAX_CORRECTION_ADJ, self.MAX_CORRECTION_ADJ)
        
        # √Åp d·ª•ng l·ª±c ƒëi·ªÅu ch·ªânh ƒë√£ ƒë∆∞·ª£c gi·ªõi h·∫°n
        left_motor = self.BASE_SPEED + adj
        right_motor = self.BASE_SPEED - adj
        self.robot.set_motors(left_motor, right_motor)
        
    def handle_intersection(self):
        rospy.loginfo("\n[GIAO L·ªò] D·ª´ng l·∫°i v√† x·ª≠ l√Ω...")
        self.robot.stop(); time.sleep(0.5)

        current_direction = self.DIRECTIONS[self.current_direction_index]
        angle_to_sign = self.ANGLE_TO_FACE_SIGN_MAP.get(current_direction, 0)
        self.turn_robot(angle_to_sign, False)
        image_info = self.latest_image
        detections = self.detect_with_yolo(image_info)
        self.turn_robot(-angle_to_sign, False)
        
        prescriptive_cmds = {det['class_name'] for det in detections if det['class_name'] in self.PRESCRIPTIVE_SIGNS}
        prohibitive_cmds = {det['class_name'] for det in detections if det['class_name'] in self.PROHIBITIVE_SIGNS}
        data_items = [det for det in detections if det['class_name'] in self.DATA_ITEMS]

        # 2. X·ª≠ l√Ω c√°c m·ª•c d·ªØ li·ªáu (QR, To√°n) v√† Publish
        rospy.loginfo("[STEP 2] Processing data items...")
        for item in data_items:
            if item['class_name'] == 'qr_code':
                # Code ƒë·ªçc QR th·∫≠t
                # box = item['box']; qr_image = self.latest_image[box[1]:box[3], box[0]:box[2]]
                # decoded = decode(qr_image)
                # if decoded: qr_data = decoded[0].data.decode('utf-8'); self.publish_data(...)
                rospy.loginfo("Found QR Code. Publishing data...")
                self.publish_data({'type': 'QR_CODE', 'value': 'simulated_data_123'})

                # response = requests.post(url, json=data)

                # print(response.status_code)

            elif item['class_name'] == 'math_problem':
                rospy.loginfo("Found Math Problem. Solving and publishing...")
                self.publish_data({'type': 'MATH_PROBLEM', 'value': '2+2=4'})
        
        
        rospy.loginfo("[STEP 3] L·∫≠p k·∫ø ho·∫°ch ƒëi·ªÅu h∆∞·ªõng theo b·∫£n ƒë·ªì...")
        # 3. L·∫≠p k·∫ø ho·∫°ch ƒêi·ªÅu h∆∞·ªõng
        final_decision = None
        is_deviation = False 

        while True:
            planned_direction_label = self.navigator.get_next_direction_label(self.current_node_id, self.planned_path)
            if not planned_direction_label:
                rospy.logerr("L·ªói k·∫ø ho·∫°ch: Kh√¥ng t√¨m th·∫•y b∆∞·ªõc ti·∫øp theo."); self._set_state(RobotState.DEAD_END); return
            
            planned_action = self.map_absolute_to_relative(planned_direction_label, current_direction)
            rospy.loginfo(f"K·∫ø ho·∫°ch A* ƒë·ªÅ xu·∫•t: ƒêi {planned_action} (h∆∞·ªõng {planned_direction_label})")

            # ∆Øu ti√™n 1: Bi·ªÉn b√°o b·∫Øt bu·ªôc
            intended_action = None
            if 'L' in prescriptive_cmds: intended_action = 'left'
            elif 'R' in prescriptive_cmds: intended_action = 'right'
            elif 'F' in prescriptive_cmds: intended_action = 'straight'
            
            # ∆Øu ti√™n 2: Plan
            if intended_action is None:
                intended_action = planned_action
            else:
                # N·∫øu h√†nh ƒë·ªông b·∫Øt bu·ªôc kh√°c v·ªõi k·∫ø ho·∫°ch, ƒë√°nh d·∫•u l√† ƒëi ch·ªách h∆∞·ªõng
                if intended_action != planned_action:
                    is_deviation = True
                    rospy.logwarn(f"CH·ªÜCH H∆Ø·ªöNG! Bi·ªÉn b√°o b·∫Øt bu·ªôc ({intended_action}) kh√°c v·ªõi k·∫ø ho·∫°ch ({planned_action}).")

            # 3.3. Veto b·ªüi bi·ªÉn b√°o c·∫•m
            is_prohibited = (intended_action == 'straight' and 'NF' in prohibitive_cmds) or \
                            (intended_action == 'right' and 'NR' in prohibitive_cmds) or \
                            (intended_action == 'left' and 'NL' in prohibitive_cmds)

            if is_prohibited:
                rospy.logwarn(f"H√†nh ƒë·ªông d·ª± ƒë·ªãnh '{intended_action}' b·ªã C·∫§M!")
                
                # N·∫øu h√†nh ƒë·ªông b·ªã c·∫•m ƒë·∫øn t·ª´ bi·ªÉn b√°o b·∫Øt bu·ªôc -> L·ªói b·∫£n ƒë·ªì
                if is_deviation:
                    rospy.logerr("L·ªñI B·∫¢N ƒê·ªí! Bi·ªÉn b√°o b·∫Øt bu·ªôc m√¢u thu·∫´n v·ªõi bi·ªÉn b√°o c·∫•m. Kh√¥ng th·ªÉ ƒëi ti·∫øp.")
                    self._set_state(RobotState.DEAD_END); return
                
                # N·∫øu h√†nh ƒë·ªông b·ªã c·∫•m ƒë·∫øn t·ª´ k·∫ø ho·∫°ch A* -> T√¨m ƒë∆∞·ªùng l·∫°i
                banned_edge = (self.current_node_id, self.planned_path[self.planned_path.index(self.current_node_id) + 1])
                if banned_edge not in self.banned_edges:
                    self.banned_edges.append(banned_edge)
                
                rospy.loginfo(f"Th√™m c·∫°nh c·∫•m {banned_edge} v√† t√¨m ƒë∆∞·ªùng l·∫°i...")
                new_path = self.navigator.find_path(self.current_node_id, self.navigator.end_node, self.banned_edges)
                
                if new_path:
                    self.planned_path = new_path
                    rospy.loginfo(f"ƒê√£ t√¨m th·∫•y ƒë∆∞·ªùng ƒëi m·ªõi: {self.planned_path}")
                    continue # Quay l·∫°i ƒë·∫ßu v√≤ng l·∫∑p ƒë·ªÉ ki·ªÉm tra v·ªõi k·∫ø ho·∫°ch m·ªõi
                else:
                    rospy.logerr("Kh√¥ng th·ªÉ t√¨m ƒë∆∞·ªùng ƒëi m·ªõi sau khi g·∫∑p bi·ªÉn c·∫•m.")
                    self._set_state(RobotState.DEAD_END); return
            
            final_decision = intended_action
            break 

        # 4. Th·ª±c thi quy·∫øt ƒë·ªãnh
        if final_decision == 'straight': 
            rospy.loginfo("[FINAL] Decision: Go STRAIGHT.")
        elif final_decision == 'right': 
            rospy.loginfo("[FINAL] Decision: Turn RIGHT.") 
            self.turn_robot(90, True)
        elif final_decision == 'left': 
            rospy.loginfo("[FINAL] Decision: Turn LEFT.") 
            self.turn_robot(-90, True)
        else:
            rospy.logwarn("[!!!] DEAD END! No valid paths found.") 
            self._set_state(RobotState.DEAD_END)
            return
        
        # 5. C·∫≠p nh·∫≠t tr·∫°ng th√°i robot sau khi th·ª±c hi·ªán
        # 5.1. X√°c ƒë·ªãnh node ti·∫øp theo
        next_node_id = None
        if not is_deviation:
            # N·∫øu ƒëi theo k·∫ø ho·∫°ch, ch·ªâ c·∫ßn l·∫•y node ti·∫øp theo t·ª´ path
            next_node_id = self.planned_path[self.planned_path.index(self.current_node_id) + 1]
        else:
            # N·∫øu ch·ªách h∆∞·ªõng, ph·∫£i t√¨m node ti·∫øp theo d·ª±a tr√™n h√†nh ƒë·ªông ƒë√£ th·ª±c hi·ªán
            
            new_robot_direction = self.DIRECTIONS[self.current_direction_index] 
            
            executed_direction_label = None
            for label, direction_enum in self.LABEL_TO_DIRECTION_ENUM.items():
                if direction_enum == new_robot_direction:
                    executed_direction_label = label 
                    break
            
            if executed_direction_label is None:
                rospy.logerr("L·ªói logic: Kh√¥ng th·ªÉ t√¨m th·∫•y label cho h∆∞·ªõng ƒëi m·ªõi c·ªßa robot."); self._set_state(RobotState.DEAD_END) 
                return

            next_node_id = self.navigator.get_neighbor_by_direction(self.current_node_id, executed_direction_label)
            if next_node_id is None:
                 rospy.logerr("L·ªñI B·∫¢N ƒê·ªí! ƒê√£ th·ª±c hi·ªán r·∫Ω nh∆∞ng kh√¥ng c√≥ node t∆∞∆°ng ·ª©ng."); self._set_state(RobotState.DEAD_END); return
            
            # Quan tr·ªçng: L·∫≠p k·∫ø ho·∫°ch l·∫°i t·ª´ v·ªã tr√≠ m·ªõi
            rospy.loginfo(f"ƒê√£ ƒëi ch·ªách k·∫ø ho·∫°ch. L·∫≠p l·∫°i ƒë∆∞·ªùng ƒëi t·ª´ node m·ªõi {next_node_id}...")
            new_path = self.navigator.find_path(next_node_id, self.navigator.end_node, self.banned_edges)
            if new_path:
                self.planned_path = new_path
                rospy.loginfo(f"ƒê∆∞·ªùng ƒëi m·ªõi sau khi ch·ªách h∆∞·ªõng: {self.planned_path}")
            else:
                rospy.logerr("Kh√¥ng th·ªÉ t√¨m ƒë∆∞·ªùng v·ªÅ ƒë√≠ch t·ª´ v·ªã tr√≠ m·ªõi."); self._set_state(RobotState.DEAD_END); return

        self.target_node_id = next_node_id
        rospy.loginfo(f"==> ƒêang di chuy·ªÉn ƒë·∫øn node ti·∫øp theo: {self.target_node_id}")
        self._set_state(RobotState.LEAVING_INTERSECTION)
    
    def turn_robot(self, degrees, update_main_direction=True):
        duration = abs(degrees) / 90.0 * self.TURN_DURATION_90_DEG
        if degrees > 0: 
            self.robot.set_motors(self.TURN_SPEED, -self.TURN_SPEED)
        elif degrees < 0: 
            self.robot.set_motors(-self.TURN_SPEED, self.TURN_SPEED)
        if degrees != 0: 
            time.sleep(duration)
        self.robot.stop()
        if update_main_direction and degrees % 90 == 0 and degrees != 0:
            num_turns = round(degrees / 90)
            self.current_direction_index = (self.current_direction_index + num_turns + 4) % 4
            rospy.loginfo(f"==> H∆∞·ªõng ƒëi M·ªöI: {self.DIRECTIONS[self.current_direction_index].name}")
        time.sleep(0.5)
    
    def _does_path_exist_in_frame(self, image):
        if image is None: return False
        roi = image[self.ROI_Y : self.ROI_Y + self.ROI_H, :]
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        mask = cv2.inRange(hsv, self.LINE_COLOR_LOWER, self.LINE_COLOR_UPPER)
        _img, contours, _hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        return bool(contours) and cv2.contourArea(max(contours, key=cv2.contourArea)) > self.SCAN_PIXEL_THRESHOLD
    
    def scan_for_available_paths_proactive(self):
        rospy.loginfo("[SCAN] B·∫Øt ƒë·∫ßu qu√©t ch·ªß ƒë·ªông...")
        paths = {"straight": False, "right": False, "left": False}
        if self.latest_image is not None:
            paths["straight"] = self._does_path_exist_in_frame(self.latest_image)
        self.turn_robot(90, update_main_direction=False); time.sleep(0.5)
        if self.latest_image is not None:
            paths["right"] = self._does_path_exist_in_frame(self.latest_image)
        self.turn_robot(-180, update_main_direction=False); time.sleep(0.5)
        if self.latest_image is not None:
            paths["left"] = self._does_path_exist_in_frame(self.latest_image)
        self.turn_robot(90, update_main_direction=False)
        rospy.loginfo(f"[SCAN] K·∫øt qu·∫£: {paths}")
        return paths

def main():
    rospy.init_node('jetbot_controller_node', anonymous=True)
    try:
        controller = JetBotController()
        controller.run()
    except rospy.ROSInterruptException: rospy.loginfo("Node ƒë√£ b·ªã ng·∫Øt.")
    except Exception as e: rospy.logerr(f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}", exc_info=True)

if __name__ == '__main__':
    main()